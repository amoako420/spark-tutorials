{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Spark Streaming\n\nSpark Streaming is an extension of core APIs that provides fault-tolerant and high-throughput processing of real-time data. It offers APIs that allow scalable processing of data streams generated from various sources.\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Understanding Batch and Stream Processing\n\n## 1. Batch Processing\nBatch processing involves applying computational logic to fixed, static datasets and producing results after processing the entire dataset. It is reliable for large-scale data but can be slow, often taking hours to complete complex jobs.\n\n## 2. Stream Processing\nStream processing handles continuous, unbounded streams of data in real time. It requires low-latency, fault-tolerant systems to manage challenges such as varying data arrival rates, maintaining correctness during failures, and processing data efficiently.\n\n## Integration of Batch and Stream Processing\nStream processing often integrates with batch processing to enrich real-time insights. For instance, live user activity streams can be joined with static profiles or historical data to enable dynamic, context-rich analytics for timely decision-making.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Use Cases of Spark Streaming\n\nSpark Streaming delivers significant value across various domains by enhancing customer experience or proactively monitoring data for actionable insights. Below are some of the prominent use cases:\n\n## 1. Fraud Detection\n- **Domain**: Financial Services  \n- **Use Case**: Detect fraudulent transactions in real time.  \n- **Value**: Enables organizations to take immediate action on suspicious transactions, minimizing potential damage.\n\n## 2. Recommendations\n- **Domain**: E-commerce, Media, and others  \n- **Use Case**: Recommend products or content based on current user activities on a platform.  \n- **Value**: Improves customer engagement and increases revenue through personalized experiences.\n\n## 3. Risk Avoidance\n- **Domain**: Service Providers  \n- **Use Case**: Expand due diligence processes to evaluate service eligibility.  \n- **Value**: Reduces the risk of providing services or products to ineligible consumers.\n\nSpark Streaming is widely adopted across industries for its ability to process real-time data efficiently and make critical decisions on the fly.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Core Concepts in Stream Processing\n\n## 1. Data Delivery Semantics\nStream processing engines guarantee data delivery under failure scenarios:\n\n- **At Most Once**: Data may be delivered zero or one time. Risk of data loss; suitable for non-critical use cases.\n- **At Least Once**: Data is delivered one or more times. Ensures no data loss but may lead to duplication (e.g., financial transactions).\n- **Exactly Once**: Data is delivered exactly one time. Prevents loss and duplication, ideal for critical business applications.\n\n### Key Insight\nDelivery semantics range from weakest `(at most once)` to strongest `(exactly once)`, with modern engines favoring **exactly once** for reliability.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. Notion of Time in Stream Processing\n\nIn the world of stream processing, the notion of time is very important because it enables\nyou to understand what\u2019s going on in terms of time. For example, in the case of a real-time\nanomaly detection application, the notion of time gives insights into the number of\nsuspicious transactions occurring in the last 5 minutes or a certain part of the day.\n\n- **Event Time**: Timestamp when data is created (e.g., IoT device logs temperature).\n- **Processing Time**: Timestamp when the stream engine processes the data.\n\n### Key Insight\n- Event time is ideal for understanding real-world events, minimizing lag impact.\n- Processing time depends on system clocks and may vary due to delays.\n- Use **event time** for accurate temporal analysis, especially with unbounded data streams.\n\nTo deal with unbounded incoming streams of data, one common practice in the stream processing engines is to divide the incoming\ndata into chunks by using the start and end time as the boundary. It makes more sense to use event time as the temporal boundaries.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# 3. Windowing in Stream Processing\n\n### Why Windowing?\n- Streaming data is unbounded; processing it in chunks is essential.\n- Example: Traffic sensor data analyzed in 1-minute or 5-minute intervals.\n\n### Windowing Patterns\n1. **Fixed/Tumbling Window**: \n   - Divides data into non-overlapping, fixed-size windows (e.g., 1 min).\n   - Ideal for straightforward aggregations like sum or average.\n\n2. **Sliding Window**: \n   - Overlapping windows with a defined slide interval.\n   - Produces smoother aggregations due to data overlap.\n\n3. **Session Window**: \n   - Dynamically determined by periods of user inactivity.\n   - Useful for analyzing user behavior (e.g., website sessions).\n   \n### Sliding Window Example: Producing Smoother Aggregations\n\nConsider a sensor that records temperature every second. Using a **fixed window** of 5 seconds, the aggregation might look like this:\n\n| Time (s) | Data                    | Fixed Window Average |\n|----------|-------------------------|----------------------|\n| 1-5      | 20, 22, 23, 21, 24       | 22                   |\n| 6-10     | 25, 27, 26, 28, 30       | 27.2                 |\n\nIn this case, the averages abruptly change between windows.\n\nNow, using a **sliding window** with a length of 5 seconds and a slide interval of 2 seconds, the overlapping windows allow a smoother transition between averages:\n\n| Time (s)   | Data                      | Sliding Window Average |\n|------------|---------------------------|------------------------|\n| 1-5        | 20, 22, 23, 21, 24         | 22                     |\n| 3-7        | 23, 21, 24, 25, 27         | 24                     |\n| 5-9        | 24, 25, 27, 26, 28         | 26                     |\n| 7-11       | 27, 26, 28, 30, 32         | 28.6                   |\n\n### Key Difference:\n- Fixed windows aggregate only distinct time chunks, causing sharp transitions.\n- Sliding windows overlap, capturing shared data points, resulting in smoother transitions between aggregates.\n\n\n### Key Insight\n- Temporal boundaries (event or processing time) define windows for meaningful analysis.\n"}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Stream Processing Engine Landscape\n\n- **Apache Storm**: Pioneer in stream processing, abandoned by Twitter in favor of Heron for better resource efficiency.\n- **Apache Samza**: Built by LinkedIn, tightly integrated with Kafka for fault-tolerant stream processing.\n- **Apache Flink**: Supports both stream and batch processing, known for high-throughput and low latency.\n- **Apache Kafka Streams**: Lightweight stream processing library on top of Kafka, easy to write real-time apps.\n- **Apache Apex**: Native Hadoop YARN platform, unifies stream and batch processing.\n- **Apache Beam**: Unified API for both stream and batch processing, portable across runtimes (Flink, Spark, DataFlow).\n\n**Processing Models:**\n- **Record-at-a-Time**: Low latency, processes each piece of data as it arrives (e.g., Apache Flink).\n- **Micro-Batching**: Higher throughput, processes data in small batches (e.g., Apache Spark).\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Data Sources for Spark Streaming\n\nSpark Streaming supports the analysis of real-time data from a variety of sources. It provides APIs to connect directly to several messaging queues and data streams, enabling seamless integration and processing. Below are some of the supported data sources:\n\n## Supported Data Sources\n1. **Kafka**  \n   - Widely used distributed messaging system.\n   - Ideal for high-throughput, fault-tolerant event streaming.\n\n2. **Flume**  \n   - Designed for collecting, aggregating, and transporting large amounts of log data.\n\n3. **Kinesis**  \n   - Amazon's real-time streaming data platform, enabling fast ingestion and processing.\n\n4. **ZeroMQ**  \n   - Lightweight messaging library designed for high-performance and low-latency messaging.\n\n5. **Twitter**  \n   - Direct integration to analyze real-time data streams from Twitter's public API.\n\nThese integrations make Spark Streaming a versatile and powerful tool for real-time data processing across multiple platforms.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Stream Processing in Spark Streaming\n\nStream processing in Spark Streaming handles real-time data streams efficiently by breaking them into **micro-batches**. These micro-batches are small chunks of data that are processed sequentially, allowing for scalable and fault-tolerant stream processing.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## DStreams (Discretized Streams)\nA **DStream** is the representation of a data stream in Spark Streaming. It is a continuous series of **RDDs** (Resilient Distributed Datasets) where:\n- Each RDD contains data from a specific time interval.\n- Operations on DStreams are automatically translated into operations on the underlying RDDs.\n\n### Key Features of DStreams\n- **Receiver Object**:  \n   Every DStream is linked to a receiver object that:\n   - Collects data from the source.\n   - Stores the data in memory for further processing.\n\nThis design makes DStreams a core abstraction for stream processing in Spark.\n"}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "### Spark Structured Streaming\n\n- **Second-Generation Streaming Engine**: Built on Spark SQL for better scalability, fault tolerance, and performance.\n\n- **Key Features**:\n  - **End-to-End Reliability**: Guarantees correctness and handles complex transformations.\n  - **Event-Time Processing**: Handles out-of-order data effectively.\n  - **Data Integration**: Supports a variety of data sources and sinks.\n\n- **Core Ideas**:\n  1. **Stream as a Table**: Treats incoming data as rows appended to a table, leveraging structured APIs (DataFrame/Dataset).\n  2. **Transactional Guarantees**: Provides end-to-end exactly-once processing and integrates with storage systems for consistent snapshots.\n\n- **Processing Models**:\n  - **Micro-Batching** (default): Low latency (~100ms), suitable for many use cases.\n  - **Continuous Processing** (experimental, Spark 2.3+): Ultra-low latency (~1ms), with some restrictions.\n\n- **Developer Benefits**:\n  - Easy transition from batch to streaming via structured APIs.\n  - Optimized using the Catalyst engine.\n  - Simplifies stream processing complexities like state maintenance and event-time handling.\n"}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Spark Structured Streaming\n\n### Overview\nSpark Structured Streaming is Spark\u2019s second-generation streaming engine, designed to handle real-time data processing with the following goals:\n- **End-to-end reliability** and **guaranteeing correctness**\n- **Complex transformations** on incoming data\n- Processing based on **event time** and handling **out-of-order data**\n- Integration with various **data sources** and **data sinks**\n\n### Core Concepts\nThe core concepts of building a streaming application in Spark include:\n- **Data Sources**: Input streams of data.\n- **Data Transformations**: Applying structured APIs to incoming data streams.\n- **Output Mode**: Defines how data is written to a sink.\n- **Trigger**: Determines when streaming computations are executed.\n- **Data Sink**: Destination for the output of streaming applications.\n\n---\n\n## Data Sources\n\nIn Spark Structured Streaming, the data sources are different from batch processing. They generate data continuously, and their rate may vary over time. Spark provides native support for the following data sources:\n\n| Data Source Type  | Description                                                                                                                                       | Fault Tolerant  |\n|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|\n| **Kafka**         | Reads from Kafka topics (requires Kafka version 0.10+). Popular in production environments.                                                       | Yes (using Kafka offset) |\n| **File**          | Reads new files dropped into directories (local file system, HDFS, or S3). Supports formats like text, CSV, JSON, Parquet, ORC.                    | Depends on file system |\n| **Socket**        | Reads UTF8 data from a socket (used for testing only).                                                                                            | No              |\n| **Rate**          | Generates events with timestamps and a monotonically increasing value (used for testing and benchmarking).                                         | No              |\n\n---\n\n## Output Modes\n\nOutput modes in Structured Streaming define how data is written to a sink:\n\n| Output Mode  | Description                                                                 |\n|--------------|-----------------------------------------------------------------------------|\n| **Append**   | Only new rows appended to the result table are written to the output sink.  |\n| **Complete** | The entire result table is written to the output sink.                      |\n| **Update**   | Only updated rows are written to the output sink. Unchanged rows are ignored. |\n\n---\n\n## Trigger Types\n\nThe trigger defines when the next batch of data will be processed in a streaming job:\n\n| Trigger Type     | Description                                                                 |\n|------------------|-----------------------------------------------------------------------------|\n| **Not Specified** | Default (micro-batch mode), Spark processes the next batch after completing the previous one. |\n| **Fixed Interval** | Processes data based on a fixed interval, regardless of the time taken for the previous batch. |\n| **One-time**      | Used for low volume, one-time processing, the application stops after processing. |\n| **Continuous**    | Low-latency, continuous processing (experimental in Spark 2.3 and later). |\n\n---\n\n## Data Sinks\n\nData sinks are where the processed data is written. Different types of sinks support different output modes:\n\n| Sink Type        | Description                                                                 | Output Modes                  | Fault Tolerance                |\n|------------------|-----------------------------------------------------------------------------|--------------------------------|--------------------------------|\n| **Kafka Sink**   | Writes data to a Kafka topic. Supports high throughput, fault tolerance.     | Append, Update, Complete       | Exactly-once semantics (with Kafka 0.10+ support) |\n| **File Sink**    | Writes data to a file system (HDFS, local, or S3). Supports various formats. | Append, Update, Complete       | Depends on underlying file system (HDFS/S3) |\n| **Foreach Sink** | Allows custom row-by-row processing, typically for external systems.        | Append, Update, Complete       | Depends on custom implementation |\n| **Console Sink** | Displays output to the console. Primarily for testing and debugging.        | Append                         | None (for debugging only)      |\n| **Memory Sink**  | Stores data in memory on the driver node. Suitable for debugging and small jobs. | Append, Update, Complete       | None (data is lost when the application stops) |\n\n---\n\n## Sink Configurations\n\nEach sink type may have its own configuration parameters. Below is a table describing key configuration options for each sink:\n\n| Sink Type        | Key Configuration Parameter(s)                    | Example Configuration                        | Notes                                      |\n|------------------|----------------------------------------------------|----------------------------------------------|--------------------------------------------|\n| **Kafka Sink**   | `kafka.bootstrap.servers`, `topic`                 | `kafka.bootstrap.servers: <broker-list>`, `topic: <topic-name>` | For connecting to Kafka brokers and topics. |\n| **File Sink**    | `path`, `format`                                   | `path: /output/directory`, `format: Parquet` | Supports formats like CSV, JSON, Parquet, Avro. |\n| **Foreach Sink** | Custom logic (depends on the use case)             | Custom row processing logic                  | Allows writing custom code for each row.  |\n| **Console Sink** | `numRows`                                          | `numRows: 10`                               | Limits the number of rows printed to the console. |\n| **Memory Sink**  | `numRows`                                          | `numRows: 1000`                              | Defines how many rows to store in memory. |\n\n---\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Lambda and Kappa Architecture\n\nWhen dealing with big data processing and analytics, architectural patterns play a crucial role in designing systems that can handle large-scale data efficiently. Two commonly used patterns are **Lambda Architecture** and **Kappa Architecture**.\n\n---\n\n## **Lambda Architecture**\n\nLambda Architecture is designed to handle massive quantities of data by utilizing both real-time and batch processing. It is particularly suitable for systems that require low-latency reads and updates.\n\n### Key Components:\n1. **Batch Layer**: Processes and stores data in a fault-tolerant manner. It computes results over the full dataset and is typically slow but highly accurate.\n2. **Speed Layer**: Handles real-time data streams for low-latency processing, typically less accurate due to approximations.\n3. **Serving Layer**: Combines outputs from both the batch and speed layers to provide a unified view of the data.\n\n### Pros:\n- Combines accuracy and low-latency processing.\n- Fault-tolerant due to the separation of batch and real-time layers.\n\n### Cons:\n- Complex to implement and maintain due to the dual-layer approach.\n- Potential duplication of logic in batch and speed layers.\n\n---\n\n## **Kappa Architecture**\n\nKappa Architecture was introduced as a simpler alternative to Lambda Architecture, focusing solely on streaming data processing. It eliminates the batch layer entirely and relies on a unified stream processing engine.\n\n### Key Components:\n1. **Stream Processing**: All data is processed as a continuous stream, ensuring near real-time analytics.\n2. **Data Store**: Stores the processed results for querying and analysis.\n\n### Pros:\n- Simpler and easier to maintain compared to Lambda Architecture.\n- Ideal for use cases where reprocessing can be achieved by replaying data streams.\n\n### Cons:\n- Not suitable for batch processing scenarios requiring historical data analysis.\n\n---\n\n## **Comparison**\n\n| Feature              | Lambda Architecture            | Kappa Architecture             |\n|----------------------|--------------------------------|--------------------------------|\n| **Complexity**       | High due to dual layers       | Low due to single layer       |\n| **Latency**          | Low (real-time layer)         | Very low (stream processing)  |\n| **Reprocessing**     | Separate batch jobs           | Stream replay                 |\n| **Use Case**         | Mixed batch and real-time     | Pure streaming use cases      |\n\n---\n\n## **Further Reading**\n\nFor an in-depth comparison and practical examples, check out this detailed blog post:  \n[Lambda vs. Kappa Architecture - Nexocode Blog](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}