{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Parallelizing a Collection in Spark\n\nParallelizing a collection is a simple way to create an **RDD (Resilient Distributed Dataset)** in Spark.  \nThis method allows you to distribute existing collections like `List`, `Array`, or `Set` across a cluster  \nfor parallel processing.\n"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "from pyspark import SparkContext\nfrom pyspark.rdd import RDD\nfrom pyspark.sql import SparkSession\n\n# Initialize SparkSession\nspark = SparkSession.builder \\\n    .appName(\"ParallelizingExample\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://spark-jupyter-m.us-central1-f.c.spark-tutorial-441812.internal:38579\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.8</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>ParallelizingExample</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f1e75c157d0>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"}], "source": "# Create an RDD by parallelizing a range of numbers\nnumberRDD = spark.sparkContext.parallelize(range(1, 10))\n\n# Collect the data from the RDD\nresult = numberRDD.collect()\n\n# Output the result\nprint(result)"}, {"cell_type": "markdown", "metadata": {}, "source": "# HOW DO WE CREATE RDDS?"}, {"cell_type": "markdown", "metadata": {}, "source": "# 1. Creating an RDD from External Datasets\n\nWhile parallelizing collections is simple, it is not suitable for large datasets. Large datasets are often stored in distributed file systems like HDFS, and Spark provides APIs like `textFile()` to read such data efficiently.  \n\nThe `textFile()` method reads a file and creates an RDD where each element represents a line from the file.\n\n### Example\nUse the file path as an argument to the `textFile()` method to create the RDD.\n"}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['', '127.0.0.1 - - [16/Nov/2024:10:00:00 +0000] \"GET /index.html HTTP/1.1\" 200 1024', '127.0.0.1 - - [16/Nov/2024:10:05:00 +0000] \"POST /login HTTP/1.1\" 302 512', '192.168.1.1 - - [16/Nov/2024:10:10:00 +0000] \"GET /dashboard HTTP/1.1\" 200 2048', '10.0.0.1 - - [16/Nov/2024:10:15:00 +0000] \"GET /profile HTTP/1.1\" 404 128']\n"}], "source": "# Define the file path\nfile_path = \"data/textfiles/sample_log.txt\"\n\n# Create an RDD by reading the text file\nlogRDD = spark.sparkContext.textFile(file_path)\n\n# Collect the data from the RDD\nresult = logRDD.collect()\n\n# Print the result\nprint(result)"}, {"cell_type": "markdown", "metadata": {}, "source": "# 2. Transforming an RDD in Spark\n\nRDDs (Resilient Distributed Datasets) are immutable, meaning they cannot be modified directly.  \nHowever, you can create a new RDD by applying transformations to an existing RDD using methods provided by Spark.\n\nFor example, you can use the `filter()` transformation to create a new RDD containing only the even or odd numbers from an existing RDD.\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[2, 4, 6, 8]\n"}], "source": "# Transform numberRDD to contain only even numbers\nevenNumberRDD = numberRDD.filter(lambda num: num % 2 == 0)\n\n# Collect and display the results\nresult = evenNumberRDD.collect()\nprint(result)"}, {"cell_type": "markdown", "metadata": {}, "source": "# 3. Creating an RDD from a DataFrame\n\nThough DataFrames are preferred for performance, converting to RDDs is useful in cases like handling unstructured data, custom partitioning after heavy computations, or integrating with legacy RDD code. Use DataFrames when possible for better efficiency.\n\nLet's create a DataFrame and convert it into an RDD:\n"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(id=1), Row(id=2), Row(id=3), Row(id=4)]"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "rangeDf = spark.range(1,5)\nrangeRDD = rangeDf.rdd\nrangeRDD.collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Counting ERROR and INFO Log Messages in a Log File\n\nNow that we have a basic understanding of how to create RDDs, let's write a simple program to read a log file and count the number of messages with log levels of `ERROR` and `INFO`.\n\nThe following code reads a log file, filters the messages with the log levels `ERROR` and `INFO`, and counts the occurrences:"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['', '2018-03-19 17:10:26 - myApp - DEBUG - debug message 1', '2018-03-19 17:10:27 - myApp - INFO - info message 1', '2018-03-19 17:10:28 - myApp - WARNING - warn message 1', '2018-03-19 17:10:29 - myApp - ERROR - error message 1', '2018-03-19 17:10:32 - myApp - CRITICAL - critical message with some error 1', '2018-03-19 17:10:33 - myApp - INFO - info message 2', '2018-03-19 17:10:37 - myApp - WARNING - warn message', '2018-03-19 17:10:41 - myApp - ERROR - error message 2']\n"}], "source": "# Define file path\nfilePath = \"data/textfiles/sample_log.log\"\n\n\n# Read the log file as an RDD\nlogRDD = spark.sparkContext.textFile(filePath)\n\n# Collect the data from the RDD\nresult1 = logRDD.collect()\nprint(result1)"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"data": {"text/plain": "[('INFO', 2), ('ERROR', 2)]"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "# Filter the lines where the log contains 'INFO' or 'ERROR'\nresultRDD = logRDD.filter(lambda line: len(line.split(\" - \")) >= 4 and line.split(\" - \")[2] in ['INFO', 'ERROR']) \\\n                  .map(lambda line: (line.split(\" - \")[2], 1)) \\\n                  .reduceByKey(lambda x, y: x + y)\n\n# Collect and print the results\nresultRDD.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Transformations in Spark with Python Code Examples\n\n## Transformations Overview\nTransformations in Spark transform existing RDDs into new RDDs. They are lazy and categorized into:\n1. **Narrow Transformations**: Data transformation without shuffle (e.g., `map`, `filter`).\n2. **Wide Transformations**: Involve shuffling data across partitions.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## Narrow Transformations"}, {"cell_type": "markdown", "metadata": {}, "source": "#### `map()`\nApplies a function to each element of an RDD, producing a new RDD with the same number of elements.\n"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/plain": "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(1, 11)).map(lambda x: x * 2).collect()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### `flatMap()`\n\nThe `flatMap()` transformation applies a function that returns an iterator to each element of an RDD. It creates a new RDD with more elements, making it useful when multiple elements are needed from a single input element. For example, an RDD containing lines can be transformed into another RDD containing individual words.\n"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/plain": "[\"It's\",\n 'fun',\n 'to',\n 'learn',\n 'Spark',\n 'This',\n 'is',\n 'a',\n 'flatMap',\n 'example',\n 'using',\n 'Python']"}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize([\"It's fun to learn Spark\", \"This is a flatMap example using Python\"])\\\n.flatMap(lambda x : x.split(\" \")).collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `filter()`\nThe `filter()` transformation applies a function to filter out elements that do not meet the specified condition. It creates a new RDD containing only the elements that satisfy the condition.\n\nExample:\nFilter numbers greater than 5 from an RDD containing numbers 1 to 10."}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[6, 7, 8, 9, 10]\n"}], "source": "# Python Example of `filter()`\n\nrdd = spark.sparkContext.parallelize(range(1, 11))\nfiltered_rdd = rdd.filter(lambda x: x > 5).collect()\n\nprint(filtered_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## `union()`\nThe `union()` transformation combines elements from two RDDs into a new RDD. It does not remove duplicates and behaves similarly to UNION ALL in SQL\n\nExample:\nCombining two RDDs containing numbers 1-5 and 5-10."}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]\n"}], "source": "# Python Example of `union()`\nfirstRDD = spark.sparkContext.parallelize(range(1, 6))\nsecondRDD = spark.sparkContext.parallelize(range(5, 11))\nunion_rdd = firstRDD.union(secondRDD).collect()\n\nprint(union_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## `mapPartitions()`\nThe `mapPartitions()` transformation provides more control by applying a function to each partition, accepting an iterator as input and returning an iterator as output.\n\nExample:\nMultiply each element by 2 using mapPartitions()"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"}], "source": "# Python Example of `mapPartitions()`\nrdd = spark.sparkContext.parallelize(range(1, 11), 2)\nresult_rdd = rdd.mapPartitions(lambda iterOfElements: [e * 2 for e in iterOfElements]).collect()\n\nprint(result_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Wide Transformations in Spark\nWide transformations involve a shuffle of data between partitions, requiring more computing resources such as memory, disk, and network bandwidth. These transformations include operations like `groupByKey()`, `reduceByKey()`, `join()`, `distinct()`, and `intersect()`. The result of a wide transformation is computed using data from multiple partitions."}, {"cell_type": "markdown", "metadata": {}, "source": "## `distinct()`\nThe `distinct()` transformation removes duplicate elements from an RDD and returns a new RDD with only unique elements."}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[1, 2, 3, 4]\n"}], "source": "# Python Example of `distinct()`\nrdd = spark.sparkContext.parallelize([1, 1, 2, 2, 3, 3, 4, 4])\ndistinct_rdd = rdd.distinct().collect()\n\nprint(distinct_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## `sortBy()`\nThe `sortBy()` transformation sorts elements in an RDD based on a given key. It allows sorting by a function applied to each element. \nIn the following example, we sort our RDD in descending order using the second element of the tuple"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('Shrey', 6), ('Rahul', 4), ('Aman', 2), ('Akash', 1)]\n"}], "source": "# Python Example of `sortBy()`\nrdd = spark.sparkContext.parallelize([('Rahul', 4), ('Aman', 2), ('Shrey', 6), ('Akash', 1)])\nsorted_rdd = rdd.sortBy(lambda x: -x[1]).collect()\n\nprint(sorted_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## `intersection()`\nThe `intersection()` transformation finds the common elements between two RDDs.\nLike union() transformation, intersection() is also a set operation between two RDDs, but involves a shuffle. The\nfollowing examples show how to find common elements between two RDDs using intersection():"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[5]\n"}], "source": "# Python Example of `intersection()`\nfirstRDD = spark.sparkContext.parallelize(range(1, 6))\nsecondRDD = spark.sparkContext.parallelize(range(5, 11))\nintersection_rdd = firstRDD.intersection(secondRDD).collect()\n\nprint(intersection_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## `subtract()`\nThe `subtract()` transformation removes elements in one RDD that exist in another RDD.\nLet's create two RDDs: The first one has numbers from 1 to 10 and the second one has elements from 6 to 10. If we\nuse subtract(), we get a new RDD with numbers 1 to 5:"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[1, 2, 3, 4, 5]\n"}], "source": "# Python Example of `subtract()`\nfirstRDD = spark.sparkContext.parallelize(range(1, 11))\nsecondRDD = spark.sparkContext.parallelize(range(6, 11))\nsubtract_rdd = firstRDD.subtract(secondRDD).collect()\n\nprint(subtract_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "## `cartesian()`\nThe `cartesian()` transformation performs a cartesian join of two RDDs, resulting in the cartesian product of both."}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[(0, 'A'), (0, 'B'), (0, 'C'), (1, 'A'), (1, 'B'), (1, 'C'), (2, 'A'), (2, 'B'), (2, 'C')]\n"}], "source": "# Python Example of `cartesian()`\nfirstRDD = spark.sparkContext.parallelize(range(3))\nsecondRDD = spark.sparkContext.parallelize(['A', 'B', 'C'])\ncartesian_rdd = firstRDD.cartesian(secondRDD).collect()\n\nprint(cartesian_rdd)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Actions in Spark\n\nActions trigger the execution of transformations and return results to the driver or external storage. They don't create new RDDs but perform operations on the existing ones. Here are some common actions:\n\n### 1. `collect()`\nReturns all elements of an RDD to the driver. Use cautiously due to memory limitations.\n\nExample:\n```python\nspark.sparkContext.parallelize(range(10)).collect()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. `count()`\nReturns the number of elements in an RDD."}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [{"data": {"text/plain": "10"}, "execution_count": 106, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(10)).count()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3. `take()`\nReturns the first N elements from an RDD"}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [{"data": {"text/plain": "[0, 1]"}, "execution_count": 44, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(10)).take(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. `top()`\nReturns the top N elements from the RDD."}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"data": {"text/plain": "[9, 8]"}, "execution_count": 47, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(10)).top(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 5. `takeOrdered()`\nReturns N elements sorted by a custom order."}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [{"data": {"text/plain": "[9, 8, 7]"}, "execution_count": 48, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(10)).takeOrdered(3, key=lambda x: -x)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6. `first()`\nReturns the first element of an RDD"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"data": {"text/plain": "0"}, "execution_count": 50, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(10)).first()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 7. `countByValue()`\nCounts occurrences of each element in the RDD."}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"data": {"text/plain": "defaultdict(int, {'A': 2, 'B': 1})"}, "execution_count": 51, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize([\"A\", \"A\", \"B\"]).countByValue()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 8. `reduce()`\nCombines RDD elements in parallel (e.g., summing values)."}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"data": {"text/plain": "55"}, "execution_count": 54, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.parallelize(range(1, 11)).reduce(lambda x, y: x + y)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Pair RDDs in Spark\nA Pair RDD is a special type of RDD in Apache Spark that contains key-value pairs. Pair RDDs are useful for a variety of operations like joins and aggregations. Spark provides optimized transformations for Pair RDDs, making them very powerful for data processing tasks.\n\n## 1. `groupByKey()`\n- The `groupByKey()` transformation groups the values of a Pair RDD based on their key. It does not perform any aggregation, it simply groups the data."}, {"cell_type": "code", "execution_count": 102, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "key - 1 , value - [5, 10]\nkey - 2 , value - [4, 6]\nkey - 3 , value - [1]\n"}], "source": "pairRDD = spark.sparkContext.parallelize([(1, 5), (1, 10), (2, 4), (3, 1), (2, 6)])\nresult = pairRDD.groupByKey().collect()\nfor pair in result:\n    print('key -', pair[0], ', value -', list(pair[1]))"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. `reduceByKey()`\n- The `reduceByKey()` transformation aggregates values by key using a specified reduction function. It minimizes the data shuffle by performing a local aggregation first and then a global aggregation across nodes."}, {"cell_type": "code", "execution_count": 101, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[(1, 15), (2, 10), (3, 1)]\n"}], "source": "pairRDD = spark.sparkContext.parallelize([(1, 5), (1, 10), (2, 4), (3, 1), (2, 6)])\nresult = pairRDD.reduceByKey(lambda x, y: x + y).collect()\nprint(result)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3. `sortByKey()`\nThe `sortByKey()` transformation sorts a Pair RDD based on its keys. By default, the sorting is in ascending order, but you can specify a custom order."}, {"cell_type": "code", "execution_count": 104, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[(1, 5), (1, 10), (2, 4), (2, 6), (3, 1)]\n"}], "source": "pairRDD = spark.sparkContext.parallelize([(1, 5), (1, 10), (2, 4), (3, 1), (2, 6)])\nsortedRDD = pairRDD.sortByKey().collect()\nprint(sortedRDD)"}, {"cell_type": "markdown", "metadata": {}, "source": "Note: You can change the sorting order by passing a custom ordering function to `sortByKey()`. e.g., `sortByKey(keyfunc=lambda k: -k)` for descending order"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. `join()`\nThe `join()` transformation joins two Pair RDDs based on their keys. It returns an RDD with the keys and the corresponding pairs of values from the two RDDs."}, {"cell_type": "code", "execution_count": 105, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('IND', (30, 300)), ('US', (20, 200))]\n"}], "source": "salesRDD = spark.sparkContext.parallelize([(\"US\", 20), (\"IND\", 30), (\"UK\", 10)])\nrevenueRDD = spark.sparkContext.parallelize([(\"US\", 200), (\"IND\", 300)])\njoinedRDD = salesRDD.join(revenueRDD).collect()\nprint(joinedRDD)"}, {"cell_type": "markdown", "metadata": {}, "source": "Other Pair RDD Transformations\n- `aggregateByKey()`: Allows performing custom aggregations for each key.\n- `cogroup()`: Groups data from two Pair RDDs based on their keys.\n- `leftOuterJoin()`, `rightOuterJoin()`: Join two RDDs with the inclusion of unmatched keys.\n- `subtractByKey()`: Removes the keys and their values from the first RDD that are present in the second RDD."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Caching and Checkpointing\n\nCaching and checkpointing are important features in Spark that can significantly improve the performance of your Spark jobs.\n\n## Caching\n\nCaching data in memory is one of the main features of Spark. You can cache large datasets in memory or on disk, depending on your cluster hardware. Caching is useful in the following scenarios:\n- When you use the same RDD multiple times.\n- To avoid recomputing an RDD that involves heavy computation, such as `join()` and `groupByKey()`.\n\nIf you need to run multiple actions on an RDD, it's a good idea to cache it in memory to avoid recomputing the same RDD. \n"}, {"cell_type": "markdown", "metadata": {}, "source": "In this example, Spark will compute the baseRDD twice to perform the `take()` and `count()` actions. \nHowever, by caching the RDD, Spark computes the RDD only once and then performs actions on the cached data. \nThis becomes more beneficial when working with large datasets, where recomputing an RDD can be expensive.\n\nSpark doesn't immediately cache the data after calling `cache()`. Instead, it makes a note of the caching operation, and once it encounters the first action, it will compute and cache the RDD based on the specified caching level."}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [{"data": {"text/plain": "10"}, "execution_count": 63, "metadata": {}, "output_type": "execute_result"}], "source": "baseRDD = spark.sparkContext.parallelize(range(1, 11))\nbaseRDD.count()"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [{"data": {"text/plain": "10"}, "execution_count": 64, "metadata": {}, "output_type": "execute_result"}], "source": "baseRDD = spark.sparkContext.parallelize(range(1, 11))\nbaseRDD.cache()  # Caching baseRDD\nbaseRDD.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Checkpointing\n\nThe life cycle of a cached RDD ends when the Spark session terminates. If you have computed an RDD and want to use it in another Spark program without recomputing it, you can use the `checkpoint()` operation. This operation stores the RDD content on disk, making it available for later use.\n\n### Example\n\nLet's go through an example of how to use checkpointing in Spark:)"}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [], "source": "baseRDD = spark.sparkContext.parallelize(['A', 'B', 'C'])\nspark.sparkContext.setCheckpointDir(\"data/tables/checkpointing\")\nbaseRDD.checkpoint()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Understanding Partitions\n\nData partitioning plays a crucial role in distributed computing, as it defines the degree of parallelism for applications. Understanding and defining partitions properly can significantly improve the performance of Spark jobs. There are two ways to control the degree of parallelism for RDD operations:\n\n- `repartition()` and `coalesce()`\n- `partitionBy()`\n\n## `repartition()` versus `coalesce()`\n\nPartitions of an existing RDD can be changed using `repartition()` or `coalesce()`. These operations redistribute the RDD based on the number of partitions provided.\n\n- **`repartition()`**: \n  - Used to increase or decrease the number of partitions.\n  - Involves heavy data shuffling across the cluster.\n  - It can be used when there is a need to increase parallelism or adjust the number of partitions based on the workload.\n  \n- **`coalesce()`**:\n  - Used to **only decrease** the number of partitions.\n  - Generally does **not trigger a shuffle**, which makes it more efficient when reducing partitions.\n  - It is ideal for optimizing execution time after heavy filtering operations. However, if the number of partitions provided is much smaller than the number of nodes in the cluster, data will be shuffled across some nodes.\n\n### Example Use Case\n\n- **When to Use `repartition()`**: If you need to increase the number of partitions significantly, for example, when your data is too large to fit in a small number of partitions and you want to take advantage of more parallelism.\n  \n- **When to Use `coalesce()`**: After filtering or when you want to optimize the performance by reducing the number of partitions (e.g., after a `filter()` operation where data is sparsely distributed across many partitions).\n\n### Summary\n\n- `repartition()` can both increase or decrease the number of partitions, but it incurs a shuffle.\n- `coalesce()` can only decrease the number of partitions, and in most cases, it avoids a shuffle, improving performance for certain workloads.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "The `repartition()` is not that bad. In some cases, when your job\nis not using all the available slots, you can repartition your data to run it\nfaster."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## `partitionBy()` in Spark\n\nThe `partitionBy()` operation in Spark allows you to control the number of partitions in an RDD, especially for operations that shuffle data, such as `groupByKey()` or `join()`. By using a partitioning function, like `HashPartitioner`, you can optimize performance by redistributing the data across partitions.\n\n#### Key Points:\n\n- **Shuffling Operations**: Operations like `groupByKey()` or `join()` can accept an additional parameter to specify the number of partitions for the resulting RDD.\n  \n- **`partitionBy()` Usage**: This operation redistributes the RDD data based on the specified partitioning function, which minimizes shuffling, especially in join operations."}, {"cell_type": "markdown", "metadata": {}, "source": "## 1. Using `groupByKey()` to control the number of partitions:\n"}, {"cell_type": "code", "execution_count": 92, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3\n"}], "source": "\n# Create an RDD with 3 partitions\nbaseRDD = spark.sparkContext.parallelize([(\"US\", 20), (\"IND\", 30), (\"UK\", 10)], 3)\n\n# Print the number of partitions\nprint(baseRDD.getNumPartitions())"}, {"cell_type": "code", "execution_count": 93, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "2\n"}], "source": "groupedRDD = baseRDD.groupByKey(2)  # Group data into 2 partitions\nprint(groupedRDD.getNumPartitions())"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. Using `partitionBy()` to control the number of partitions:"}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "2\n"}], "source": "# Create an RDD with 3 partitions\nbaseRDD = spark.sparkContext.parallelize([(\"US\", 20), (\"IND\", 30), (\"UK\", 10)], 3)\n\n# Repartition the RDD using partitionBy with 2 partitions\npartitionedRDD = baseRDD.partitionBy(2)\n\n# Persist the partitioned RDD if it's going to be used frequently\npartitionedRDD.persist()\n\n# Check the number of partitions after partitionBy\nprint(partitionedRDD.getNumPartitions())\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Drawbacks of Using RDDs\n\n- **Opaque Code**: RDD code can sometimes be unclear, making it difficult for developers to understand what exactly the code is computing.\n  \n- **Lack of Optimizations**: Spark cannot optimize RDDs as effectively, especially when it comes to lambda functions. For example, Spark might not perform operations like `filter()` before wide transformations (e.g., `reduceByKey()` or `groupByKey()`), even when it could improve performance.\n\n- **Performance on Non-JVM Languages**: RDDs are slower when used in non-JVM languages like Python and R. This is due to the overhead of creating a separate virtual machine for Python/R alongside the JVM, which involves data transfer between the VMs, significantly increasing execution time. \n\n- **No Built-in Optimizations**: Unlike DataFrames, RDDs do not benefit from Spark\u2019s Catalyst optimizer, leading to potentially suboptimal execution plans.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# DataFrames in Spark\n\nDataFrames are an abstraction of RDD APIs and provide a more efficient way to process structured data. They are distributed collections of data organized into rows and columns, similar to tables in a relational database. \nDataFrames enable users to perform data processing on data from various sources, including RDDs, different file formats, and databases.\n\n## Features of DataFrames\n\n- **Support for Various Data Sources**: DataFrames can process data from a wide range of sources, such as:\n  - Files in different formats (CSV, AVRO, JSON, etc.)\n  - Storage systems (Hive, HDFS, RDBMS)\n  \n- **Scalability**: DataFrames can handle data volumes ranging from kilobytes to petabytes, making them suitable for both small and large datasets.\n\n- **Optimized Query Processing**: DataFrames leverage the Spark SQL query optimizer, which ensures that data processing is performed efficiently and distributed across multiple nodes.\n\n- **Multi-Language API Support**: DataFrames support APIs in multiple languages, including:\n  - Java\n  - Scala\n  - Python\n  - R"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Running SQL on DataFrames in Spark\n\nIn Spark, you can run SQL queries on DataFrames by creating temporary views. These views can be either **local** or **global**.\n\n## Temporary Views on DataFrames\n\nTemporary views allow you to run SQL queries within a single session. After creating a temporary view, you can run SQL queries directly on the DataFrame and get the result as another DataFrame.\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'sales_df' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-4-e8693bbca201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a temporary view on the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sales\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run SQL query on the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msqlDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM sales\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'sales_df' is not defined"]}], "source": "# Create a temporary view on the DataFrame\nsales_df.createOrReplaceTempView(\"sales\")\n\n# Run SQL query on the DataFrame\nsqlDF = spark.sql(\"SELECT * FROM sales\")\n\n# Show the results\nsqlDF.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 5}